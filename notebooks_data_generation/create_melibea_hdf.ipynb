{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e3aed8f",
   "metadata": {},
   "source": [
    "Import packages and setting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6115991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- packages --- #\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os, sys, glob\n",
    "import uproot\n",
    "pd.set_option(\"display.max_columns\", None) # to show all columns when displaying dataframes\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# --- logging --- #\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# --- scripts paths --- #\n",
    "sys.path.insert(0, '/fefs/aswg/workspace/juan.jimenez/stereo_analysis/scripts')\n",
    "import auxiliar as aux\n",
    "import find_files\n",
    "aux.params()\n",
    "\n",
    "# --- other parameters --- #\n",
    "# name of the source we are studying\n",
    "source_name = 'BLLac'\n",
    "# ------------------------ #\n",
    "\n",
    "# --- file paths --- #\n",
    "# common data file\n",
    "common_data = f'/fefs/aswg/workspace/juan.jimenez/stereo_analysis/config_files/common_data{source_name}.txt'\n",
    "# output directory for melibea files\n",
    "output_dir = '/fefs/aswg/workspace/juan.jimenez/data/dl2/melibea'\n",
    "# name of the total merged file that contain the coincident event ids\n",
    "path_merged = f'/fefs/aswg/workspace/juan.jimenez/data/dl2/stereo_merged_{source_name}/dl2_merged_{source_name}_total.3tel.h5'\n",
    "# ------------------ #\n",
    "\n",
    "logger.info(f'Study of the source: {source_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7dc39b",
   "metadata": {},
   "source": [
    "## Extracting `obs_ids` we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60ba13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting all observation ids\n",
    "run_strings = np.loadtxt(common_data, dtype='str')\n",
    "\n",
    "run_id_LST   = [int(f.split('-')[0]) for f in run_strings] \n",
    "run_id_MAGIC = [str(f.split('-')[1]) for f in run_strings] \n",
    "run_id_MAGIC = np.sort(list(dict.fromkeys([int(i)  for r in run_id_MAGIC for i in r.split(',')])))\n",
    "\n",
    "# finding .root directories\n",
    "files_MAGIC = find_files.find_MAGIC_melibea(run_id_MAGIC)\n",
    "\n",
    "runs_MAGIC = []\n",
    "for file in files_MAGIC:\n",
    "    index = file.find('/Melibea/')\n",
    "    \n",
    "    if index == -1:\n",
    "        index_alt = file.find('/mars_q/')\n",
    "        runs_MAGIC.append(int(file[index_alt+8+9:index_alt+8+9+8]))\n",
    "        \n",
    "    else:\n",
    "        runs_MAGIC.append(int(file[index + 29 : index + 29 + 8]))\n",
    "        \n",
    "logger.info(f'\\nSelected {len(runs_MAGIC)} runs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ff40a4",
   "metadata": {},
   "source": [
    "Extracting also the event ids que want. We only are interested in the coincident ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d8d4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_merged = glob.glob(path_merged)[0]\n",
    "df_merged =  pd.read_hdf(dir_merged, key='events/parameters')\n",
    "\n",
    "logger.info(f'The merged dl2 ({sys.getsizeof(df_merged)*1e-9:.1f}Gb):')\n",
    "display(df_merged.head(5))\n",
    "\n",
    "magic_ids = np.unique(df_merged['magic_id'].to_numpy())\n",
    "logger.info(f'\\nThe amount of events available for stereo analysis is {len(magic_ids)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f143620e",
   "metadata": {},
   "source": [
    "# Extracting the data we want from `melibea`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fb7c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stereo evevent id or header\n",
    "event_id = 'MRawEvtHeader_1.fStereoEvtNumber'\n",
    "\n",
    "# size or intensity recorded by both telescopes\n",
    "intensity_M1 = 'MHillas_1.fSize'\n",
    "intensity_M2 = 'MHillas_2.fSize'\n",
    "\n",
    "# source positions for M1 and M2\n",
    "src_pos_M1_X = 'MSrcPosCam_1.fX'\n",
    "src_pos_M1_Y = 'MSrcPosCam_1.fY'\n",
    "src_pos_M2_X = 'MSrcPosCam_2.fX'\n",
    "src_pos_M2_Y = 'MSrcPosCam_2.fY'\n",
    "\n",
    "# impact parameters\n",
    "imp_par_1    = 'MStereoParDisp.fM1Impact'\n",
    "imp_par_1_Az = 'MStereoParDisp.fM1ImpactAz'\n",
    "imp_par_2    = 'MStereoParDisp.fM2Impact'\n",
    "imp_par_2_Az = 'MStereoParDisp.fM2ImpactAz'\n",
    "\n",
    "# stereo reconstruction parameters with, and without disp method\n",
    "st_dirX = 'MStereoParDisp.fDirectionX'\n",
    "st_dirY = 'MStereoParDisp.fDirectionY'\n",
    "\n",
    "# pointing positions\n",
    "pointing_ra  = 'MPointingPos_1.fRa'\n",
    "pointing_dec  = 'MPointingPos_1.fDec'\n",
    "pointing_alt = 'MPointingPos_1.fZd'\n",
    "pointing_az  = 'MPointingPos_1.fAz'\n",
    "\n",
    "# recovered energy and hadroness\n",
    "reco_energy = 'MEnergyEst.fEnergy'\n",
    "hadroness   = 'MHadronness.fHadronness'\n",
    "\n",
    "# dec and re coord \n",
    "reco_dec = 'MStereoParDisp.fDirectionDec'\n",
    "reco_ra  = 'MStereoParDisp.fDirectionRA' \n",
    "\n",
    "# timestamp in ns\n",
    "timestamp = 'MRawEvtHeader_1.fClockCounter'\n",
    "delta_t   = 'MRawEvtHeader_1.fTimeDiff'\n",
    "\n",
    "# impact parameter and height\n",
    "h_max   = 'MStereoParDisp.fMaxHeight'\n",
    "slope_1 = 'MHillas_1.fDelta'\n",
    "slope_2 = 'MHillas_2.fDelta'\n",
    "\n",
    "# creating the dataframe with the needed modifications of variables, standarizing everything\n",
    "melibea_runs_df = []\n",
    "for file in files_MAGIC:\n",
    "    \n",
    "    # opening the file with uproot\n",
    "    myFile = uproot.open(file)\n",
    "\n",
    "    # finding the number of the key needed (because changes in each run)\n",
    "    root_label = myFile.keys()[2]\n",
    "\n",
    "    # appending the data labels and names of the columsn for the dataframe\n",
    "    dataL = [ event_id ,  intensity_M1 ,  intensity_M2 ,  src_pos_M1_X ,  src_pos_M1_Y ,  src_pos_M2_X]\n",
    "    names = ['event_id', 'intensity_M1', 'intensity_M2', 'src_pos_M1_X', 'src_pos_M1_Y', 'src_pos_M2_X']\n",
    "    dataL = [*dataL, *[ src_pos_M2_Y ,  imp_par_1 ,  imp_par_1_Az ,  imp_par_2 ,  imp_par_2_Az ]]\n",
    "    names = [*names, *['src_pos_M2_Y', 'imp_par_1', 'imp_par_1_Az', 'imp_par_2', 'imp_par_2_Az']]\n",
    "    dataL = [*dataL, *[ st_dirX ,  st_dirY ,  reco_energy ,  hadroness ,  reco_dec ,  reco_ra,   pointing_alt ,  pointing_az ]]\n",
    "    names = [*names, *['st_dirX', 'st_dirY', 'reco_energy', 'gammaness', 'reco_dec', 'reco_ra', 'pointing_alt', 'pointing_az']]\n",
    "    dataL = [*dataL, *[ h_max ,  slope_1 ,  slope_2 ,  pointing_ra ,  pointing_dec ,  timestamp ,  delta_t ]]\n",
    "    names = [*names, *['h_max', 'slope_1', 'slope_2', 'pointing_ra', 'pointing_dec', 'timestamp', 'delta_t']]\n",
    "\n",
    "    # data matrix\n",
    "    data  = np.array([np.array(myFile[root_label][d].array()) for d in dataL])\n",
    "\n",
    "    # create the dataframe\n",
    "    df = pd.DataFrame(data.T, columns=names)\n",
    "\n",
    "    # add obs_ids\n",
    "    df['obs_id'] = runs_MAGIC[files_MAGIC.index(file)]\n",
    "    \n",
    "    # convert to int the event id's\n",
    "    df['event_id']  = df['event_id'].astype('int')\n",
    "    \n",
    "    # convert hadroness to gammaness\n",
    "    df['gammaness'] = 1. - df['gammaness']\n",
    "    \n",
    "    # convert MeV to TeV\n",
    "    df['reco_energy'] = df['reco_energy'].to_numpy() * 1e-3\n",
    "    \n",
    "    # converting coordinates from hours to degrees\n",
    "    df['reco_ra']      = df['reco_ra'] * 15\n",
    "    df['pointing_ra']  = df['pointing_ra'] * 15\n",
    "    df['pointing_alt'] = 90 - df['pointing_alt']\n",
    "    \n",
    "    # adding a magic total id\n",
    "    df['magic_id']    = np.char.add(df['obs_id'].to_numpy().astype(str), np.char.add('.',df['event_id'].to_numpy().astype(str))) \n",
    "\n",
    "    # finding if there are no reconstructed events\n",
    "    nonreco = len(df.query(f'reco_energy == -0.001', inplace=False))\n",
    "    \n",
    "    fname_index = [i for i in range(len(file)) if file.startswith('/', i)][-1]\n",
    "    logger.info(f'For {file[fname_index:]} found {nonreco} non-reconstructed events ({100 * nonreco / len(df):.2f}%)')\n",
    "    \n",
    "    melibea_runs_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5fbac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the melibea dataframes for each run\n",
    "melibea_runs_df = pd.concat(melibea_runs_df)\n",
    "\n",
    "# now we filter for the coincident events\n",
    "melibea_runs_df.query(f'magic_id in @magic_ids', inplace=True)\n",
    "\n",
    "# sort the indexes of the dataframe\n",
    "melibea_runs_df.set_index(['obs_id', 'event_id'], inplace=True)\n",
    "melibea_runs_df.sort_index(inplace=True)\n",
    "\n",
    "# create directory\n",
    "aux.createdir(output_dir)\n",
    "# filename\n",
    "fname = os.path.join(output_dir, f'dl2_melibea_{source_name}.h5')\n",
    "# converting to .h5\n",
    "melibea_runs_df.to_hdf(fname, key='/events/parameters')\n",
    "logger.info(f'A total dataframe of {len(melibea_runs_df)} events is created at:\\n--> {fname}')\n",
    "\n",
    "# printing the dataframe also\n",
    "display(melibea_runs_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150213a2",
   "metadata": {},
   "source": [
    "## Check the different keys we have inside `melibea.root` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e73660",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# opening with uproot\n",
    "myFile = uproot.open(files_MAGIC[0])\n",
    "# extracting primary key, that changes in each dataset\n",
    "primary_key = myFile.keys()[2]\n",
    "\n",
    "# opening all the keys\n",
    "all_keys = myFile[primary_key].keys()\n",
    "\n",
    "# and showing them\n",
    "all_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dbe85c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
